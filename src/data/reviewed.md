# LLM interpretability papers

## Open Problems in Mechanistic Interpretability

**Link:** https://arxiv.org/pdf/2501.16496



## On the Biology of a Large Language Model

**Link:** https://transformer-circuits.pub/2025/attribution-graphs/biology.html



## Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning

**Link:** https://arxiv.org/pdf/2510.02091

**Core claim:**  
How each layer of LLM is used for different tasks using layer pruning. Shallow layers for retrieval, deep layers for reasoning.

> Results show that layer contributions are highly uneven: shallow layers dominate likelihood
> and retrieval, while mid-to-deep layers are essential for reasoning and generation. Taken together, depth usage
> is inherently task-dependent, highly metric-sensitive, and strongly model-specific. 


## Revisiting Hallucination Detection with Effective Rank-based Uncertainty

**Link:** https://arxiv.org/pdf/2510.08389v1

**Core claim:**  
High uncertainty / variance of internal hidden-state vectors shows a high correlation with hallucination.


## Memory Retrieval and Consolidation in Large Language Models through Function Tokens

**Link:** https://arxiv.org/pdf/2510.08203v1

**Core claim:**  
Function tokens: punctuation, articles, prepositions, etc. 
...

https://arxiv.org/pdf/2510.07613v1

http://arxiv.org/pdf/2510.08120v1


# Other papers

## How do LLMs Acquire New Knowledge? A Knowledge Circuits Perspective on Continual Pre-Training

**Link:** https://arxiv.org/pdf/2502.11196



## Can LLMs Generate Novel Research Ideas?

**Link:** https://arxiv.org/pdf/2409.04109

**Core claim:**  
LLMs create more novel ideas than expert human researchers.
LLMs produce less feasible ideas than researchers.
LLMs lack diversity in their ideas, and cannot reliably self-evaluate.