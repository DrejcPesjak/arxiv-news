Top 3 (LLM interpretability focus)
- AbsTopK: Rethinking Sparse Autoencoders For Bidirectional Features — advances SAEs to capture bidirectional concepts, improving feature-level mechanistic interpretability. Link: http://arxiv.org/pdf/2510.00404v2
- Towards Interpretable and Inference-Optimal COT Reasoning with Sparse Autoencoder-Guided Generation — uses SAEs to analyze internal token representations and guide reasoning, connecting features to behavior. Link: http://arxiv.org/pdf/2510.01528v1
- Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning — systematic layer-depth analysis clarifying where reasoning vs knowledge live, informing interpretability and compression. Link: http://arxiv.org/pdf/2510.02091v1

Brief bracket winners by batch
- Batch 1: Demystifying the Roles of LLM Layers...
- Batch 2: The Unseen Frontier (LLM sparsity via ADMM)
- Batch 3: SAE-Guided Generation (interpetable CoT)
- Batch 5: Interpreting Language Models Through Concept Descriptions (survey)
- Batch 7: Analyzing Latent Concepts in Code LMs
- Batch 8: AbsTopK (bidirectional SAEs)

Final pick rationale
- AbsTopK and SAE-Guided Generation push state of the art in feature-level interpretability and link features to reasoning behavior.
- Layer Roles paper provides strong empirical grounding for where to probe/steer models.