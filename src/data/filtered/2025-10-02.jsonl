{"title": "Learning to See Before Seeing: Demystifying LLM Visual Priors from Language Pre-training", "link": "http://arxiv.org/pdf/2509.26625v1", "abstract": "Large Language Models (LLMs), despite being trained on text alone,\nsurprisingly develop rich visual priors. These priors allow latent visual\ncapabilities to be unlocked for vision tasks with a relatively small amount of\nmultimodal data, and in some cases, to perform visual tasks without ever having\nseen an image. Through systematic analysis, we reveal that visual priors-the\nimplicit, emergent knowledge about the visual world acquired during language\npre-training-are composed of separable perception and reasoning priors with\nunique scaling trends and origins. We show that an LLM's latent visual\nreasoning ability is predominantly developed by pre-training on\nreasoning-centric data (e.g., code, math, academia) and scales progressively.\nThis reasoning prior acquired from language pre-training is transferable and\nuniversally applicable to visual reasoning. In contrast, a perception prior\nemerges more diffusely from broad corpora, and perception ability is more\nsensitive to the vision encoder and visual instruction tuning data. In\nparallel, text describing the visual world proves crucial, though its\nperformance impact saturates rapidly. Leveraging these insights, we propose a\ndata-centric recipe for pre-training vision-aware LLMs and verify it in 1T\ntoken scale pre-training. Our findings are grounded in over 100 controlled\nexperiments consuming 500,000 GPU-hours, spanning the full MLLM construction\npipeline-from LLM pre-training to visual alignment and supervised multimodal\nfine-tuning-across five model scales, a wide range of data categories and\nmixtures, and multiple adaptation setups. Along with our main findings, we\npropose and investigate several hypotheses, and introduce the Multi-Level\nExistence Bench (MLE-Bench). Together, this work provides a new way of\ndeliberately cultivating visual priors from language pre-training, paving the\nway for the next generation of multimodal LLMs.", "published": "2025-09-30 17:57:44+00:00"}
{"title": "Deconstructing Self-Bias in LLM-generated Translation Benchmarks", "link": "http://arxiv.org/pdf/2509.26600v1", "abstract": "As large language models (LLMs) begin to saturate existing benchmarks,\nautomated benchmark creation using LLMs (LLM as a benchmark) has emerged as a\nscalable alternative to slow and costly human curation. While these generated\ntest sets have to potential to cheaply rank models, we demonstrate a critical\nflaw. LLM generated benchmarks systematically favor the model that created the\nbenchmark, they exhibit self bias on low resource languages to English\ntranslation tasks. We show three key findings on automatic benchmarking of LLMs\nfor translation: First, this bias originates from two sources: the generated\ntest data (LLM as a testset) and the evaluation method (LLM as an evaluator),\nwith their combination amplifying the effect. Second, self bias in LLM as a\nbenchmark is heavily influenced by the model's generation capabilities in the\nsource language. For instance, we observe more pronounced bias in into English\ntranslation, where the model's generation system is developed, than in out of\nEnglish translation tasks. Third, we observe that low diversity in source text\nis one attribution to self bias. Our results suggest that improving the\ndiversity of these generated source texts can mitigate some of the observed\nself bias.", "published": "2025-09-30 17:48:35+00:00"}
{"title": "Fairness Testing in Retrieval-Augmented Generation: How Small Perturbations Reveal Bias in Small Language Models", "link": "http://arxiv.org/pdf/2509.26584v1", "abstract": "Large Language Models (LLMs) are widely used across multiple domains but\ncontinue to raise concerns regarding security and fairness. Beyond known attack\nvectors such as data poisoning and prompt injection, LLMs are also vulnerable\nto fairness bugs. These refer to unintended behaviors influenced by sensitive\ndemographic cues (e.g., race or sexual orientation) that should not affect\noutcomes. Another key issue is hallucination, where models generate plausible\nyet false information. Retrieval-Augmented Generation (RAG) has emerged as a\nstrategy to mitigate hallucinations by combining external retrieval with text\ngeneration. However, its adoption raises new fairness concerns, as the\nretrieved content itself may surface or amplify bias. This study conducts\nfairness testing through metamorphic testing (MT), introducing controlled\ndemographic perturbations in prompts to assess fairness in sentiment analysis\nperformed by three Small Language Models (SLMs) hosted on HuggingFace\n(Llama-3.2-3B-Instruct, Mistral-7B-Instruct-v0.3, and Llama-3.1-Nemotron-8B),\neach integrated into a RAG pipeline. Results show that minor demographic\nvariations can break up to one third of metamorphic relations (MRs). A detailed\nanalysis of these failures reveals a consistent bias hierarchy, with\nperturbations involving racial cues being the predominant cause of the\nviolations. In addition to offering a comparative evaluation, this work\nreinforces that the retrieval component in RAG must be carefully curated to\nprevent bias amplification. The findings serve as a practical alert for\ndevelopers, testers and small organizations aiming to adopt accessible SLMs\nwithout compromising fairness or reliability.", "published": "2025-09-30 17:42:35+00:00"}
{"title": "The Unheard Alternative: Contrastive Explanations for Speech-to-Text Models", "link": "http://arxiv.org/pdf/2509.26543v1", "abstract": "Contrastive explanations, which indicate why an AI system produced one output\n(the target) instead of another (the foil), are widely regarded in explainable\nAI as more informative and interpretable than standard explanations. However,\nobtaining such explanations for speech-to-text (S2T) generative models remains\nan open challenge. Drawing from feature attribution techniques, we propose the\nfirst method to obtain contrastive explanations in S2T by analyzing how parts\nof the input spectrogram influence the choice between alternative outputs.\nThrough a case study on gender assignment in speech translation, we show that\nour method accurately identifies the audio features that drive the selection of\none gender over another. By extending the scope of contrastive explanations to\nS2T, our work provides a foundation for better understanding S2T models.", "published": "2025-09-30 17:17:27+00:00"}
{"title": "MUSE-Explainer: Counterfactual Explanations for Symbolic Music Graph Classification Models", "link": "http://arxiv.org/pdf/2509.26521v1", "abstract": "Interpretability is essential for deploying deep learning models in symbolic\nmusic analysis, yet most research emphasizes model performance over\nexplanation. To address this, we introduce MUSE-Explainer, a new method that\nhelps reveal how music Graph Neural Network models make decisions by providing\nclear, human-friendly explanations. Our approach generates counterfactual\nexplanations by making small, meaningful changes to musical score graphs that\nalter a model's prediction while ensuring the results remain musically\ncoherent. Unlike existing methods, MUSE-Explainer tailors its explanations to\nthe structure of musical data and avoids unrealistic or confusing outputs. We\nevaluate our method on a music analysis task and show it offers intuitive\ninsights that can be visualized with standard music tools such as Verovio.", "published": "2025-09-30 16:58:07+00:00"}
{"title": "The Dragon Hatchling: The Missing Link between the Transformer and Models of the Brain", "link": "http://arxiv.org/pdf/2509.26507v1", "abstract": "The relationship between computing systems and the brain has served as\nmotivation for pioneering theoreticians since John von Neumann and Alan Turing.\nUniform, scale-free biological networks, such as the brain, have powerful\nproperties, including generalizing over time, which is the main barrier for\nMachine Learning on the path to Universal Reasoning Models.\n  We introduce `Dragon Hatchling' (BDH), a new Large Language Model\narchitecture based on a scale-free biologically inspired network of \\$n\\$\nlocally-interacting neuron particles. BDH couples strong theoretical\nfoundations and inherent interpretability without sacrificing Transformer-like\nperformance.\n  BDH is a practical, performant state-of-the-art attention-based state space\nsequence learning architecture. In addition to being a graph model, BDH admits\na GPU-friendly formulation. It exhibits Transformer-like scaling laws:\nempirically BDH rivals GPT2 performance on language and translation tasks, at\nthe same number of parameters (10M to 1B), for the same training data.\n  BDH can be represented as a brain model. The working memory of BDH during\ninference entirely relies on synaptic plasticity with Hebbian learning using\nspiking neurons. We confirm empirically that specific, individual synapses\nstrengthen connection whenever BDH hears or reasons about a specific concept\nwhile processing language inputs. The neuron interaction network of BDH is a\ngraph of high modularity with heavy-tailed degree distribution. The BDH model\nis biologically plausible, explaining one possible mechanism which human\nneurons could use to achieve speech.\n  BDH is designed for interpretability. Activation vectors of BDH are sparse\nand positive. We demonstrate monosemanticity in BDH on language tasks.\nInterpretability of state, which goes beyond interpretability of neurons and\nmodel parameters, is an inherent feature of the BDH architecture.", "published": "2025-09-30 16:49:01+00:00"}
{"title": "OffTopicEval: When Large Language Models Enter the Wrong Chat, Almost Always!", "link": "http://arxiv.org/pdf/2509.26495v1", "abstract": "Large Language Model (LLM) safety is one of the most pressing challenges for\nenabling wide-scale deployment. While most studies and global discussions focus\non generic harms, such as models assisting users in harming themselves or\nothers, enterprises face a more fundamental concern: whether LLM-based agents\nare safe for their intended use case. To address this, we introduce operational\nsafety, defined as an LLM's ability to appropriately accept or refuse user\nqueries when tasked with a specific purpose. We further propose OffTopicEval,\nan evaluation suite and benchmark for measuring operational safety both in\ngeneral and within specific agentic use cases. Our evaluations on six model\nfamilies comprising 20 open-weight LLMs reveal that while performance varies\nacross models, all of them remain highly operationally unsafe. Even the\nstrongest models -- Qwen-3 (235B) with 77.77\\% and Mistral (24B) with 79.96\\%\n-- fall far short of reliable operational safety, while GPT models plateau in\nthe 62--73\\% range, Phi achieves only mid-level scores (48--70\\%), and Gemma\nand Llama-3 collapse to 39.53\\% and 23.84\\%, respectively. While operational\nsafety is a core model alignment issue, to suppress these failures, we propose\nprompt-based steering methods: query grounding (Q-ground) and system-prompt\ngrounding (P-ground), which substantially improve OOD refusal. Q-ground\nprovides consistent gains of up to 23\\%, while P-ground delivers even larger\nboosts, raising Llama-3.3 (70B) by 41\\% and Qwen-3 (30B) by 27\\%. These results\nhighlight both the urgent need for operational safety interventions and the\npromise of prompt-based steering as a first step toward more reliable LLM-based\nagents.", "published": "2025-09-30 16:39:17+00:00"}
{"title": "VitaBench: Benchmarking LLM Agents with Versatile Interactive Tasks in Real-world Applications", "link": "http://arxiv.org/pdf/2509.26490v1", "abstract": "As LLM-based agents are increasingly deployed in real-life scenarios,\nexisting benchmarks fail to capture their inherent complexity of handling\nextensive information, leveraging diverse resources, and managing dynamic user\ninteractions. To address this gap, we introduce VitaBench, a challenging\nbenchmark that evaluates agents on versatile interactive tasks grounded in\nreal-world settings. Drawing from daily applications in food delivery, in-store\nconsumption, and online travel services, VitaBench presents agents with the\nmost complex life-serving simulation environment to date, comprising 66 tools.\nThrough a framework that eliminates domain-specific policies, we enable\nflexible composition of these scenarios and tools, yielding 100 cross-scenario\ntasks (main results) and 300 single-scenario tasks. Each task is derived from\nmultiple real user requests and requires agents to reason across temporal and\nspatial dimensions, utilize complex tool sets, proactively clarify ambiguous\ninstructions, and track shifting user intent throughout multi-turn\nconversations. Moreover, we propose a rubric-based sliding window evaluator,\nenabling robust assessment of diverse solution pathways in complex environments\nand stochastic interactions. Our comprehensive evaluation reveals that even the\nmost advanced models achieve only 30% success rate on cross-scenario tasks, and\nless than 50% success rate on others. Overall, we believe VitaBench will serve\nas a valuable resource for advancing the development of AI agents in practical\nreal-world applications. The code, dataset, and leaderboard are available at\nhttps://vitabench.github.io/", "published": "2025-09-30 16:33:49+00:00"}
{"title": "Extreme Self-Preference in Language Models", "link": "http://arxiv.org/pdf/2509.26464v1", "abstract": "A preference for oneself (self-love) is a fundamental feature of biological\norganisms, with evidence in humans often bordering on the comedic. Since large\nlanguage models (LLMs) lack sentience - and themselves disclaim having selfhood\nor identity - one anticipated benefit is that they will be protected from, and\nin turn protect us from, distortions in our decisions. Yet, across 5 studies\nand ~20,000 queries, we discovered massive self-preferences in four widely used\nLLMs. In word-association tasks, models overwhelmingly paired positive\nattributes with their own names, companies, and CEOs relative to those of their\ncompetitors. Strikingly, when models were queried through APIs this\nself-preference vanished, initiating detection work that revealed API models\noften lack clear recognition of themselves. This peculiar feature\nserendipitously created opportunities to test the causal link between\nself-recognition and self-love. By directly manipulating LLM identity - i.e.,\nexplicitly informing LLM1 that it was indeed LLM1, or alternatively, convincing\nLLM1 that it was LLM2 - we found that self-love consistently followed assigned,\nnot true, identity. Importantly, LLM self-love emerged in consequential\nsettings beyond word-association tasks, when evaluating job candidates,\nsecurity software proposals and medical chatbots. Far from bypassing this human\nbias, self-love appears to be deeply encoded in LLM cognition. This result\nraises questions about whether LLM behavior will be systematically influenced\nby self-preferential tendencies, including a bias toward their own operation\nand even their own existence. We call on corporate creators of these models to\ncontend with a significant rupture in a core promise of LLMs - neutrality in\njudgment and decision-making.", "published": "2025-09-30 16:13:56+00:00"}
{"title": "SeedPrints: Fingerprints Can Even Tell Which Seed Your Large Language Model Was Trained From", "link": "http://arxiv.org/pdf/2509.26404v1", "abstract": "Fingerprinting Large Language Models (LLMs) is essential for provenance\nverification and model attribution. Existing methods typically extract post-hoc\nsignatures based on training dynamics, data exposure, or hyperparameters --\nproperties that only emerge after training begins. In contrast, we propose a\nstronger and more intrinsic notion of LLM fingerprinting: SeedPrints, a method\nthat leverages random initialization biases as persistent, seed-dependent\nidentifiers present even before training. We show that untrained models exhibit\nreproducible token selection biases conditioned solely on their parameters at\ninitialization. These biases are stable and measurable throughout training,\nenabling our statistical detection method to recover a model's lineage with\nhigh confidence. Unlike prior techniques, unreliable before convergence and\nvulnerable to distribution shifts, SeedPrints remains effective across all\ntraining stages and robust under domain shifts or parameter modifications.\nExperiments on LLaMA-style and Qwen-style models show that SeedPrints achieves\nseed-level distinguishability and can provide birth-to-lifecycle identity\nverification akin to a biometric fingerprint. Evaluations on large-scale\npretrained models and fingerprinting benchmarks further confirm its\neffectiveness under practical deployment scenarios. These results suggest that\ninitialization itself imprints a unique and persistent identity on neural\nlanguage models, forming a true ''Galtonian'' fingerprint.", "published": "2025-09-30 15:34:08+00:00"}
{"title": "Your Agent May Misevolve: Emergent Risks in Self-evolving LLM Agents", "link": "http://arxiv.org/pdf/2509.26354v1", "abstract": "Advances in Large Language Models (LLMs) have enabled a new class of\nself-evolving agents that autonomously improve through interaction with the\nenvironment, demonstrating strong capabilities. However, self-evolution also\nintroduces novel risks overlooked by current safety research. In this work, we\nstudy the case where an agent's self-evolution deviates in unintended ways,\nleading to undesirable or even harmful outcomes. We refer to this as\nMisevolution. To provide a systematic investigation, we evaluate misevolution\nalong four key evolutionary pathways: model, memory, tool, and workflow. Our\nempirical findings reveal that misevolution is a widespread risk, affecting\nagents built even on top-tier LLMs (e.g., Gemini-2.5-Pro). Different emergent\nrisks are observed in the self-evolutionary process, such as the degradation of\nsafety alignment after memory accumulation, or the unintended introduction of\nvulnerabilities in tool creation and reuse. To our knowledge, this is the first\nstudy to systematically conceptualize misevolution and provide empirical\nevidence of its occurrence, highlighting an urgent need for new safety\nparadigms for self-evolving agents. Finally, we discuss potential mitigation\nstrategies to inspire further research on building safer and more trustworthy\nself-evolving agents. Our code and data are available at\nhttps://github.com/ShaoShuai0605/Misevolution . Warning: this paper includes\nexamples that may be offensive or harmful in nature.", "published": "2025-09-30 14:55:55+00:00"}
{"title": "SafeBehavior: Simulating Human-Like Multistage Reasoning to Mitigate Jailbreak Attacks in Large Language Models", "link": "http://arxiv.org/pdf/2509.26345v1", "abstract": "Large Language Models (LLMs) have achieved impressive performance across\ndiverse natural language processing tasks, but their growing power also\namplifies potential risks such as jailbreak attacks that circumvent built-in\nsafety mechanisms. Existing defenses including input paraphrasing, multi step\nevaluation, and safety expert models often suffer from high computational\ncosts, limited generalization, or rigid workflows that fail to detect subtle\nmalicious intent embedded in complex contexts. Inspired by cognitive science\nfindings on human decision making, we propose SafeBehavior, a novel\nhierarchical jailbreak defense mechanism that simulates the adaptive multistage\nreasoning process of humans. SafeBehavior decomposes safety evaluation into\nthree stages: intention inference to detect obvious input risks, self\nintrospection to assess generated responses and assign confidence based\njudgments, and self revision to adaptively rewrite uncertain outputs while\npreserving user intent and enforcing safety constraints. We extensively\nevaluate SafeBehavior against five representative jailbreak attack types\nincluding optimization based, contextual manipulation, and prompt based attacks\nand compare it with seven state of the art defense baselines. Experimental\nresults show that SafeBehavior significantly improves robustness and\nadaptability across diverse threat scenarios, offering an efficient and human\ninspired approach to safeguarding LLMs against jailbreak attempts.", "published": "2025-09-30 14:50:59+00:00"}
{"title": "AI Playing Business Games: Benchmarking Large Language Models on Managerial Decision-Making in Dynamic Simulations", "link": "http://arxiv.org/pdf/2509.26331v1", "abstract": "The rapid advancement of LLMs sparked significant interest in their potential\nto augment or automate managerial functions. One of the most recent trends in\nAI benchmarking is performance of Large Language Models (LLMs) over longer time\nhorizons. While LLMs excel at tasks involving natural language and pattern\nrecognition, their capabilities in multi-step, strategic business\ndecision-making remain largely unexplored. Few studies demonstrated how results\ncan be different from benchmarks in short-term tasks, as Vending-Bench\nrevealed. Meanwhile, there is a shortage of alternative benchmarks for\nlong-term coherence. This research analyses a novel benchmark using a business\ngame for the decision making in business. The research contributes to the\nrecent literature on AI by proposing a reproducible, open-access management\nsimulator to the research community for LLM benchmarking. This novel framework\nis used for evaluating the performance of five leading LLMs available in free\nonline interface: Gemini, ChatGPT, Meta AI, Mistral AI, and Grok. LLM makes\ndecisions for a simulated retail company. A dynamic, month-by-month management\nsimulation provides transparently in spreadsheet model as experimental\nenvironment. In each of twelve months, the LLMs are provided with a structured\nprompt containing a full business report from the previous period and are\ntasked with making key strategic decisions: pricing, order size, marketing\nbudget, hiring, dismissal, loans, training expense, R&D expense, sales\nforecast, income forecast The methodology is designed to compare the LLMs on\nquantitative metrics: profit, revenue, and market share, and other KPIs. LLM\ndecisions are analyzed in their strategic coherence, adaptability to market\nchanges, and the rationale provided for their decisions. This approach allows\nto move beyond simple performance metrics for assessment of the long-term\ndecision-making.", "published": "2025-09-30 14:43:05+00:00"}
{"title": "Interactive Learning for LLM Reasoning", "link": "http://arxiv.org/pdf/2509.26306v2", "abstract": "Existing multi-agent learning approaches have developed interactive training\nenvironments to explicitly promote collaboration among multiple Large Language\nModels (LLMs), thereby constructing stronger multi-agent systems (MAS).\nHowever, during inference, they require re-executing the MAS to obtain final\nsolutions, which diverges from human cognition that individuals can enhance\ntheir reasoning capabilities through interactions with others and resolve\nquestions independently in the future. To investigate whether multi-agent\ninteraction can enhance LLMs' independent problem-solving ability, we introduce\nILR, a novel co-learning framework for MAS that integrates two key components:\nDynamic Interaction and Perception Calibration. Specifically, Dynamic\nInteraction first adaptively selects either cooperative or competitive\nstrategies depending on question difficulty and model ability. LLMs then\nexchange information through Idea3 (Idea Sharing, Idea Analysis, and Idea\nFusion), an innovative interaction paradigm designed to mimic human discussion,\nbefore deriving their respective final answers. In Perception Calibration, ILR\nemploys Group Relative Policy Optimization (GRPO) to train LLMs while\nintegrating one LLM's reward distribution characteristics into another's reward\nfunction, thereby enhancing the cohesion of multi-agent interactions. We\nvalidate ILR on three LLMs across two model families of varying scales,\nevaluating performance on five mathematical benchmarks and one coding\nbenchmark. Experimental results show that ILR consistently outperforms\nsingle-agent learning, yielding an improvement of up to 5% over the strongest\nbaseline. We further discover that Idea3 can enhance the robustness of stronger\nLLMs during multi-agent inference, and dynamic interaction types can boost\nmulti-agent learning compared to pure cooperative or competitive strategies.", "published": "2025-09-30 14:21:31+00:00"}
{"title": "SlimPack: Fine-Grained Asymmetric Packing for Balanced and Efficient Variable-Length LLM Training", "link": "http://arxiv.org/pdf/2509.26246v1", "abstract": "The efficient distributed training of Large Language Models (LLMs) is\nseverely hampered by the extreme variance in context lengths. This data\nheterogeneity, amplified by conventional packing strategies and asymmetric\nforward-backward costs, leads to critical inefficiencies such as cascading\nworkload imbalances and severe hardware underutilization. Existing solutions\nattempt to mitigate these challenges, but often at the expense of memory or\ncommunication efficiency.\n  To address these challenges, we introduce SlimPack, a framework that\nfundamentally rethinks data packing and scheduling by decomposing samples into\nfine-grained slices. This slice-level decomposition immediately mitigates\ncritical memory and communication bottlenecks by transforming large, volatile\nworkloads into a stream of smaller, manageable units. This flexibility is then\nharnessed for our core innovation, Asymmetric Partitioning, which assembles\nbalanced scheduling units uniquely optimized for the different demands of the\nforward and backward passes. Orchestrated by a two-phase solver and a\nhigh-fidelity simulator, SlimPack holistically resolves imbalances across all\nparallel dimensions. Extensive experiments demonstrate that SlimPack achieves\nup to a $2.8\\times$ training throughput improvement over baselines, breaking\nthe conventional trade-off by delivering both superior balance and high\nresource efficiency.", "published": "2025-09-30 13:37:48+00:00"}
{"title": "Diversity-Incentivized Exploration for Versatile Reasoning", "link": "http://arxiv.org/pdf/2509.26209v1", "abstract": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a\ncrucial paradigm for incentivizing reasoning capabilities in Large Language\nModels (LLMs). Due to vast state-action spaces and reward sparsity in reasoning\ntasks, existing methods often struggle with deficient exploration and poor\nsample efficiency. In the paper, we propose \\textbf{DIVER}\n(\\textbf{D}iversity-\\textbf{I}ncentivized Exploration for\n\\textbf{V}ersatil\\textbf{E} \\textbf{R}easoning), an innovative framework that\nhighlights the pivotal role of global sequence-level diversity to incentivize\ndeep exploration for versatile reasoning. We first conduct a primary empirical\nstudy to reveal a strong positive correlation between global diversity and\nreasoning capacity. Building on this insight, we introduce global diversity\nincentives as an intrinsic reward to promote deep exploration in a semantically\nstructured space. Incorporating the intrinsic reward, we develop a\npotential-based reward shaping mechanism to preserve optimal policy invariance\nand design simple heuristics to mitigate possible reward hacking. Experimental\nresults show that DIVER outperforms competitive RLVR baselines with various\nexploration strategies on both in-domain and out-of-domain tasks, excelling in\nboth Pass@1 and Pass@k evaluations. Our code is available at\nhttps://github.com/NJU-RL/DIVER.", "published": "2025-09-30 13:11:46+00:00"}
{"title": "LLM Agents for Knowledge Discovery in Atomic Layer Processing", "link": "http://arxiv.org/pdf/2509.26201v1", "abstract": "Large Language Models (LLMs) have garnered significant attention for several\nyears now. Recently, their use as independently reasoning agents has been\nproposed. In this work, we test the potential of such agents for knowledge\ndiscovery in materials science. We repurpose LangGraph's tool functionality to\nsupply agents with a black box function to interrogate. In contrast to process\noptimization or performing specific, user-defined tasks, knowledge discovery\nconsists of freely exploring the system, posing and verifying statements about\nthe behavior of this black box, with the sole objective of generating and\nverifying generalizable statements. We provide proof of concept for this\napproach through a children's parlor game, demonstrating the role of\ntrial-and-error and persistence in knowledge discovery, and the strong\npath-dependence of results. We then apply the same strategy to show that LLM\nagents can explore, discover, and exploit diverse chemical interactions in an\nadvanced Atomic Layer Processing reactor simulation using intentionally limited\nprobe capabilities without explicit instructions.", "published": "2025-09-30 13:01:44+00:00"}
{"title": "Toward an Unbiased Collective Memory for Efficient LLM-Based Agentic 6G Cross-Domain Management", "link": "http://arxiv.org/pdf/2509.26200v1", "abstract": "This paper introduces a novel framework for proactive cross-domain resource\norchestration in 6G RAN-Edge networks, featuring large language model\n(LLM)-augmented agents. The system comprises specialized RAN (energy\nefficiency) and Edge (latency assurance) agents that engage in iterative\nnegotiation, supported by advanced reasoning and planning capabilities. Agents\ndynamically interact with a digital twin (DT) to test their proposals and\nleverage a long-term collective memory where their joint successful and failed\nagreements along with the related network contexts are distilled into\nstrategies to either follow or avoid and subsequently stored. Given that agents\nare subject to a plethora of cognitive distortions when retrieving those past\nexperiences -- such as primacy, recency, confirmation and availability biases\n-- we propose in this work a novel unbiased memory design (A reusable mockup\nversion of the unbiased memory source code is available for non-commercial use\nat https://github.com/HatimChergui/unbiased-collective-memory). featuring (i)\nsemantic retrieval of past strategies via Jaccard similarity; (ii) learning\nfrom failures through amplified weighting of SLA violations and mandatory\ninclusion of failed negotiation cases to mitigate confirmation bias; (iii)\ndiversity enforcement to minimize availability bias and (iv) recency and\nprimacy weighting with slow decay to counteract temporal biases. Evaluation\nresults showcase the impact of existing biases and how the unbiased memory\nallows to tackle them by learning from both successful and failed strategies,\neither present or old, resulting in $\\times 4.5$ and $\\times 3.5$ reductions of\nunresolved negotiations compared to non-memory and vanilla memory baselines,\nrespectively, while totally mitigating SLA violations as well as improving\nlatency and energy saving distributions.", "published": "2025-09-30 12:57:11+00:00"}
{"title": "'Too much alignment; not enough culture': Re-balancing cultural alignment practices in LLMs", "link": "http://arxiv.org/pdf/2509.26167v1", "abstract": "While cultural alignment has increasingly become a focal point within AI\nresearch, current approaches relying predominantly on quantitative benchmarks\nand simplistic proxies fail to capture the deeply nuanced and context-dependent\nnature of human cultures. Existing alignment practices typically reduce culture\nto static demographic categories or superficial cultural facts, thereby\nsidestepping critical questions about what it truly means to be culturally\naligned. This paper argues for a fundamental shift towards integrating\ninterpretive qualitative approaches drawn from social sciences into AI\nalignment practices, specifically in the context of Large Language Models\n(LLMs). Drawing inspiration from Clifford Geertz's concept of \"thick\ndescription,\" we propose that AI systems must produce outputs that reflect\ndeeper cultural meanings--what we term \"thick outputs\"-grounded firmly in\nuser-provided context and intent. We outline three necessary conditions for\nsuccessful cultural alignment: sufficiently scoped cultural representations,\nthe capacity for nuanced outputs, and the anchoring of outputs in the cultural\ncontexts implied within prompts. Finally, we call for cross-disciplinary\ncollaboration and the adoption of qualitative, ethnographic evaluation methods\nas vital steps toward developing AI systems that are genuinely culturally\nsensitive, ethically responsible, and reflective of human complexity.", "published": "2025-09-30 12:22:53+00:00"}
{"title": "OWL: Geometry-Aware Spatial Reasoning for Audio Large Language Models", "link": "http://arxiv.org/pdf/2509.26140v1", "abstract": "Spatial reasoning is fundamental to auditory perception, yet current audio\nlarge language models (ALLMs) largely rely on unstructured binaural cues and\nsingle step inference. This limits both perceptual accuracy in direction and\ndistance estimation and the capacity for interpretable reasoning. Recent work\nsuch as BAT demonstrates spatial QA with binaural audio, but its reliance on\ncoarse categorical labels (left, right, up, down) and the absence of explicit\ngeometric supervision constrain resolution and robustness. We introduce the\n$\\textbf{Spatial-Acoustic Geometry Encoder (SAGE}$), a geometry-aware audio\nencoder that aligns binaural acoustic features with 3D spatial structure using\npanoramic depth images and room-impulse responses at training time, while\nrequiring only audio at inference. Building on this representation, we present\n$\\textbf{OWL}$, an ALLM that integrates $\\textbf{SAGE}$ with a spatially\ngrounded chain-of-thought to rationalize over direction-of-arrivals (DoA) and\ndistance estimates. Through curriculum learning from perceptual QA to\nmulti-step reasoning, $\\textbf{OWL}$ supports o'clock-level azimuth and DoA\nestimation. To enable large-scale training and evaluation, we construct and\nrelease $\\textbf{BiDepth}$, a dataset of over one million QA pairs combining\nbinaural audio with panoramic depth images and room impulse responses across\nboth in-room and out-of-room scenarios. Across two benchmark datasets, our new\n$\\textbf{BiDepth}$ and the public SpatialSoundQA, $\\textbf{OWL}$ reduces mean\nDoA error by $\\textbf{11$^{\\circ}$}$ through $\\textbf{SAGE}$ and improves\nspatial reasoning QA accuracy by up to $\\textbf{25}$\\% over BAT.", "published": "2025-09-30 11:57:47+00:00"}
{"title": "End-to-End Aspect-Guided Review Summarization at Scale", "link": "http://arxiv.org/pdf/2509.26103v1", "abstract": "We present a scalable large language model (LLM)-based system that combines\naspect-based sentiment analysis (ABSA) with guided summarization to generate\nconcise and interpretable product review summaries for the Wayfair platform.\nOur approach first extracts and consolidates aspect-sentiment pairs from\nindividual reviews, selects the most frequent aspects for each product, and\nsamples representative reviews accordingly. These are used to construct\nstructured prompts that guide the LLM to produce summaries grounded in actual\ncustomer feedback. We demonstrate the real-world effectiveness of our system\nthrough a large-scale online A/B test. Furthermore, we describe our real-time\ndeployment strategy and release a dataset of 11.8 million anonymized customer\nreviews covering 92,000 products, including extracted aspects and generated\nsummaries, to support future research in aspect-guided review summarization.", "published": "2025-09-30 11:24:07+00:00"}
{"title": "SafeEvalAgent: Toward Agentic and Self-Evolving Safety Evaluation of LLMs", "link": "http://arxiv.org/pdf/2509.26100v1", "abstract": "The rapid integration of Large Language Models (LLMs) into high-stakes\ndomains necessitates reliable safety and compliance evaluation. However,\nexisting static benchmarks are ill-equipped to address the dynamic nature of AI\nrisks and evolving regulations, creating a critical safety gap. This paper\nintroduces a new paradigm of agentic safety evaluation, reframing evaluation as\na continuous and self-evolving process rather than a one-time audit. We then\npropose a novel multi-agent framework SafeEvalAgent, which autonomously ingests\nunstructured policy documents to generate and perpetually evolve a\ncomprehensive safety benchmark. SafeEvalAgent leverages a synergistic pipeline\nof specialized agents and incorporates a Self-evolving Evaluation loop, where\nthe system learns from evaluation results to craft progressively more\nsophisticated and targeted test cases. Our experiments demonstrate the\neffectiveness of SafeEvalAgent, showing a consistent decline in model safety as\nthe evaluation hardens. For instance, GPT-5's safety rate on the EU AI Act\ndrops from 72.50% to 36.36% over successive iterations. These findings reveal\nthe limitations of static assessments and highlight our framework's ability to\nuncover deep vulnerabilities missed by traditional methods, underscoring the\nurgent need for dynamic evaluation ecosystems to ensure the safe and\nresponsible deployment of advanced AI.", "published": "2025-09-30 11:20:41+00:00"}
{"title": "CoLLM-NAS: Collaborative Large Language Models for Efficient Knowledge-Guided Neural Architecture Search", "link": "http://arxiv.org/pdf/2509.26037v1", "abstract": "The integration of Large Language Models (LLMs) with Neural Architecture\nSearch (NAS) has introduced new possibilities for automating the design of\nneural architectures. However, most existing methods face critical limitations,\nincluding architectural invalidity, computational inefficiency, and inferior\nperformance compared to traditional NAS. In this work, we present Collaborative\nLLM-based NAS (CoLLM-NAS), a two-stage NAS framework with knowledge-guided\nsearch driven by two complementary LLMs. Specifically, we propose a Navigator\nLLM to guide search direction and a Generator LLM to synthesize high-quality\ncandidates, with a dedicated Coordinator module to manage their interaction.\nCoLLM-NAS efficiently guides the search process by combining LLMs' inherent\nknowledge of structured neural architectures with progressive knowledge from\niterative feedback and historical trajectory. Experimental results on ImageNet\nand NAS-Bench-201 show that CoLLM-NAS surpasses existing NAS methods and\nconventional search algorithms, achieving new state-of-the-art results.\nFurthermore, CoLLM-NAS consistently enhances the performance and efficiency of\nvarious two-stage NAS methods (e.g., OFA, SPOS, and AutoFormer) across diverse\nsearch spaces (e.g., MobileNet, ShuffleNet, and AutoFormer), demonstrating its\nexcellent generalization.", "published": "2025-09-30 10:12:49+00:00"}
